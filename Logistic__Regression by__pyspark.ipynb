{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This codes were written by pysaprk for  creating a Logestic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Create the Spark session object\n",
    "spark=SparkSession.builder.appName('logRe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset using DataFrame\n",
    "df=spark.read.csv('Log_Reg_dataset.csv',inferSchema=True,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 6)\n"
     ]
    }
   ],
   "source": [
    "#Shape of Dataset\n",
    "print((df.count(),len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Repeat_Visitor: integer (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Web_pages_viewed: integer (nullable = true)\n",
      " |-- Status: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check and confirm the datatype of data, might you want to change the types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Country', 'Age', 'Repeat_Visitor', 'Platform', 'Web_pages_viewed', 'Status']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show name of columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+--------+----------------+------+\n",
      "|  Country|Age|Repeat_Visitor|Platform|Web_pages_viewed|Status|\n",
      "+---------+---+--------------+--------+----------------+------+\n",
      "|    India| 41|             1|   Yahoo|              21|     1|\n",
      "|   Brazil| 28|             1|   Yahoo|               5|     0|\n",
      "|   Brazil| 40|             0|  Google|               3|     0|\n",
      "|Indonesia| 31|             1|    Bing|              15|     1|\n",
      "| Malaysia| 32|             0|  Google|              15|     1|\n",
      "+---------+---+--------------+--------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Look at the Dataframe using the show function\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------------+-----------------+--------+-----------------+------------------+\n",
      "|summary| Country|              Age|   Repeat_Visitor|Platform| Web_pages_viewed|            Status|\n",
      "+-------+--------+-----------------+-----------------+--------+-----------------+------------------+\n",
      "|  count|   20000|            20000|            20000|   20000|            20000|             20000|\n",
      "|   mean|    null|         28.53955|           0.5029|    null|           9.5533|               0.5|\n",
      "| stddev|    null|7.888912950773227|0.500004090187782|    null|6.073903499824976|0.5000125004687693|\n",
      "|    min|  Brazil|               17|                0|    Bing|                1|                 0|\n",
      "|    max|Malaysia|              111|                1|   Yahoo|               29|                 1|\n",
      "+-------+--------+-----------------+-----------------+--------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Statistical measures of the dataset\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  Country|count|\n",
      "+---------+-----+\n",
      "| Malaysia| 1218|\n",
      "|    India| 4018|\n",
      "|Indonesia|12178|\n",
      "|   Brazil| 2586|\n",
      "+---------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|Platform|count|\n",
      "+--------+-----+\n",
      "|   Yahoo| 9859|\n",
      "|    Bing| 4360|\n",
      "|  Google| 5781|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+------------------+-------------------+---------------------+------------------+\n",
      "|Platform|          avg(Age)|avg(Repeat_Visitor)|avg(Web_pages_viewed)|       avg(Status)|\n",
      "+--------+------------------+-------------------+---------------------+------------------+\n",
      "|   Yahoo|28.569226087838523| 0.5094837204584644|    9.599655137437875|0.5071508266558474|\n",
      "|    Bing| 28.68394495412844| 0.4720183486238532|    9.114908256880733|0.4559633027522936|\n",
      "|  Google|28.380038055699707| 0.5149628092025601|    9.804878048780488|0.5210171250648676|\n",
      "+--------+------------------+-------------------+---------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use groupby function along with  different function returns the frequency of each of the categories in the data to understand deeper the details of each column,\n",
    "df.groupBy('Country').count().show()\n",
    "df.groupBy('Platform').count().show()\n",
    "df.groupBy('Platform').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "from pyspark.ml.feature import StringIndexer # For converting the categorical variable into numerical\n",
    "from pyspark.ml.feature import VectorAssembler #For combining all the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since we are dealing with two categorical columns, we will have to convert the country and Platform columns into numerical form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first step is to label the column using StringIndexer into numerical form. It allocates unique values to each of the categories of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_engine_indexer =StringIndexer(inputCol=\"Platform\", outputCol=\"Platform_Num\").fit(df)\n",
    "df = search_engine_indexer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+--------+----------------+------+------------+\n",
      "|Country|Age|Repeat_Visitor|Platform|Web_pages_viewed|Status|Platform_Num|\n",
      "+-------+---+--------------+--------+----------------+------+------------+\n",
      "|India  |41 |1             |Yahoo   |21              |1     |0.0         |\n",
      "|Brazil |28 |1             |Yahoo   |5               |0     |0.0         |\n",
      "|Brazil |40 |0             |Google  |3               |0     |1.0         |\n",
      "+-------+---+--------------+--------+----------------+------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Platform|count|\n",
      "+--------+-----+\n",
      "|Yahoo   |9859 |\n",
      "|Google  |5781 |\n",
      "|Bing    |4360 |\n",
      "+--------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|Platform_Num|count|\n",
      "+------------+-----+\n",
      "|0.0         |9859 |\n",
      "|1.0         |5781 |\n",
      "|2.0         |4360 |\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the details of Platform column and Platform_Num column\n",
    "df.groupBy('Platform').count().orderBy('count', ascending=False).show(5,False)\n",
    "df.groupBy('Platform_Num').count().orderBy('count', ascending=False).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next step is to represent each of these values into the form of a one hot encoded vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "search_engine_encoder=OneHotEncoder(inputCol=\"Platform_Num\", outputCol=\"Platform_Vector\")\n",
    "df = search_engine_encoder.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Letâ€™s repeat the same procedure for the other categorical column(Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_indexer = StringIndexer(inputCol=\"Country\",outputCol=\"Country_Num\").fit(df)\n",
    "df = country_indexer.transform(df)\n",
    "country_encoder = OneHotEncoder(inputCol=\"Country_Num\",outputCol=\"Country_Vector\")\n",
    "df = country_encoder.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have converted both the categorical columns into numerical forms, we need to assemble all of the input columns into a single vector that would act as the input feature for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['Platform_Vector','Country_Vector','Age', 'Repeat_Visitor',\n",
    "'Web_pages_viewed'], outputCol=\"features\")\n",
    "df = df_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------+\n",
      "|features                           |Status|\n",
      "+-----------------------------------+------+\n",
      "|[1.0,0.0,0.0,1.0,0.0,41.0,1.0,21.0]|1     |\n",
      "|[1.0,0.0,0.0,0.0,1.0,28.0,1.0,5.0] |0     |\n",
      "|(8,[1,4,5,7],[1.0,1.0,40.0,3.0])   |0     |\n",
      "|(8,[2,5,6,7],[1.0,31.0,1.0,15.0])  |1     |\n",
      "|(8,[1,5,7],[1.0,32.0,15.0])        |1     |\n",
      "|(8,[1,4,5,7],[1.0,1.0,32.0,3.0])   |0     |\n",
      "|(8,[1,4,5,7],[1.0,1.0,32.0,6.0])   |0     |\n",
      "|(8,[1,2,5,7],[1.0,1.0,27.0,9.0])   |0     |\n",
      "|(8,[0,2,5,7],[1.0,1.0,32.0,2.0])   |0     |\n",
      "|(8,[2,5,6,7],[1.0,31.0,1.0,16.0])  |1     |\n",
      "+-----------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['features','Status']).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us select only features column as input and the Status column as output for training the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df=df.select(['features','Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15070\n",
      "4930\n"
     ]
    }
   ],
   "source": [
    "training_df,test_df=model_df.randomSplit([0.75,0.25])\n",
    "print(training_df.count())\n",
    "print(test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "log_reg=LogisticRegression(labelCol='Status').fit(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For evauating our model and follow what our model predicted lables \n",
    "train_results=log_reg.evaluate(training_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|            features|Status|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.93130796852951...|[0.99735202334563...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.93130796852951...|[0.99735202334563...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.93130796852951...|[0.99735202334563...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.93130796852951...|[0.99735202334563...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.93130796852951...|[0.99735202334563...|       0.0|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------------------------------+\n",
      "|Status|prediction|probability                              |\n",
      "+------+----------+-----------------------------------------+\n",
      "|1     |1.0       |[0.3085128352044721,0.691487164795528]   |\n",
      "|1     |1.0       |[0.3085128352044721,0.691487164795528]   |\n",
      "|1     |1.0       |[0.1742514347904196,0.8257485652095804]  |\n",
      "|1     |1.0       |[0.09075088014880064,0.9092491198511994] |\n",
      "|1     |1.0       |[0.09075088014880064,0.9092491198511994] |\n",
      "|1     |1.0       |[0.09075088014880064,0.9092491198511994] |\n",
      "|1     |1.0       |[0.09075088014880064,0.9092491198511994] |\n",
      "|1     |1.0       |[0.09075088014880064,0.9092491198511994] |\n",
      "|1     |1.0       |[0.045079054372010874,0.9549209456279892]|\n",
      "|1     |1.0       |[0.045079054372010874,0.9549209456279892]|\n",
      "+------+----------+-----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter lables which our model predicted correctly as 1(label=1) and selected three columns 'Status','prediction','probability'\n",
    "train_results.filter(train_results['Status']==1).filter(train_results['prediction']==1).select(['Status','prediction','probability']).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Linear Regression Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=log_reg.evaluate(test_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Status|prediction|\n",
      "+------+----------+\n",
      "|0     |0.0       |\n",
      "|0     |0.0       |\n",
      "|0     |0.0       |\n",
      "|0     |0.0       |\n",
      "|0     |0.0       |\n",
      "|0     |0.0       |\n",
      "|0     |0.0       |\n",
      "|0     |0.0       |\n",
      "|1     |0.0       |\n",
      "|0     |0.0       |\n",
      "+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(['Status','prediction']).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will manually create the variables for true positives, true negatives, false positives, and false negatives to understand them better rather than using the direct inbuilt function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=results.filter((results['Status']==1) & (results['prediction']==1)).count()\n",
    "tn=results.filter((results['Status']==0) & (results['prediction']==0)).count()\n",
    "fp=results.filter((results['Status']==0) & (results['prediction']==1)).count()\n",
    "fn=results.filter((results['Status']==1) & (results['prediction']==0)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy= 0.9356997971602434\n"
     ]
    }
   ],
   "source": [
    "accuracy=float((tp+tn) /(results.count()))\n",
    "print(\"Accurcy=\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall= 0.9313287281592848\n"
     ]
    }
   ],
   "source": [
    "recall =float((tp) /(tp+fn))\n",
    "print(\"Recall=\",recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision= 0.9393442622950819\n"
     ]
    }
   ],
   "source": [
    "Precision =float((tp) /(tp+fp))\n",
    "print(\"Precision=\",Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
